{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hlI1rYKa2IGx"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RN8N3O43QDT5"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/prompt_optimizer/vertex_ai_prompt_optimizer_ui.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fprompts%2Fprompt_optimizer%2Fvertex_ai_prompt_optimizer_ui.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/prompts/prompt_optimizer/vertex_ai_prompt_optimizer_ui.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/prompt_optimizer/vertex_ai_prompt_optimizer_ui.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHyuJTFr2IGx"
   },
   "source": [
    "# Overview\n",
    "Welcome to Vertex AI Prompt Optimizer (VAPO)! This Notebook showcases VAPO, a tool that iteratively optimizes prompts to suit a target model (e.g., `gemini-1.5-pro`) using target-specific metric(s).\n",
    "\n",
    "Key Use Cases:\n",
    "\n",
    "* Prompt Optimization: Enhance the quality of an initial prompt by refining its structure and content to match the target model's optimal input characteristics.\n",
    "\n",
    "* Prompt Translation: Adapt prompts optimized for one model to work effectively with a different target model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTtKHedrO1Rx"
   },
   "source": [
    "# Step 0: Install packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-Zw72vFORz_"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/prompts/prompt_optimizer/vapo_lib.py\n",
    "import vapo_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-p59jd5rOp4q"
   },
   "source": [
    "# Step 1: Configure your prompt template\n",
    "Provide your system intruction and prompt template below. Refer to [LINK] for instructions.\n",
    "\n",
    "Prompts consist of two key components:\n",
    "* System Instruction: System instruction is the instruction that get passed to the model before any user input in the prompt. This is the fixed part of the prompt template shared across all queries for a given task.\n",
    "* Prompt template: A task is the text in the prompt that you want the model to provide a response for. Context is information that you include in the prompt that the model uses or references when generating a response. These are the dynamic parts of the prompt template that changes based on the task.\n",
    "\n",
    "Prompt Optimizer enables the optimization or translation of the System Instruction template, while the prompt template remains essential for evaluating and selecting the best System Instruction template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJG1pVZO317x"
   },
   "outputs": [],
   "source": [
    "SYSTEM_INSTRUCTION = \"Answer the following question. Let's think step by step.\\n\"  # @param {type:\"string\"}\n",
    "PROMPT_TEMPLATE = (\n",
    "    \"Question: {{question}}\\n\\nAnswer:{{target}}\"  # @param {type:\"string\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5y-cmg0TQP6v"
   },
   "source": [
    "# Step 2: Input your data\n",
    "To optimize the prompt for your target Google model, provide a CSV or JSONL file containing labeled validation samples (input, groud truth output pairs).\n",
    "* Focus on examples that specifically demonstrate the issues you want to address.\n",
    "* Recommendation: Use 50-100 distinct samples for reliable results. However, the tool can still be effective with as few as 5 samples.\n",
    "\n",
    "For prompt translation (e.g. 3P model to Google model, PaLM 2 to Gemini):\n",
    "* Consider using the source model to label examples that the target model struggles with, helping to identify areas for improvement.\n",
    "* When you select a source model, you don't need to provide labels for the input examples.\n",
    "* While the source model selection is limited to Google models, it still supports labeled inputs from non-Google models. If you wish to select a non-Google source model, you will need to provide labels for your input examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfgi_oR6tTIB"
   },
   "outputs": [],
   "source": [
    "# @markdown **Project setup**: <br/>\n",
    "PROJECT_ID = \"[YOUR_PROJECT]\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "OUTPUT_PATH = \"[OUTPUT_PATH]\"  # @param {type:\"string\"}\n",
    "# @markdown * GCS path of your bucket, e.g., gs://prompt_translation_demo, used to store all artifacts.\n",
    "INPUT_DATA_PATH = \"[INPUT_DATA_PATH]\"  # @param {type:\"string\"}\n",
    "# @markdown * Specify a GCS path for the input data, e.g., gs://prompt_translation_demo/input_data.jsonl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucebZHkHRxKH"
   },
   "source": [
    "# Step 3: Configure optimization settings\n",
    "The optimization configurations are defaulted to the values that are most commonly used, which we recommend using as the initial set-up.\n",
    "\n",
    "The most important settings are:\n",
    "*   Target Model: Which model you are trying to optimize your prompts to.\n",
    "*   Optimization Mode: The mode in which you are trying to optimize your prompt with.\n",
    "*   Evaluation Metrics: The evaluation metrics in which you are trying to optimize your prompts against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2R3P8mMvK9q"
   },
   "outputs": [],
   "source": [
    "SOURCE_MODEL = \"\"  # @param [\"\", \"gemini-1.0-pro-001\", \"gemini-1.0-pro-002\", \"gemini-1.5-flash-001\", \"gemini-1.5-pro-001\", \"gemini-1.0-ultra-001\", \"text-bison@001\", \"text-bison@002\", \"text-bison32k@002\", \"text-unicorn@001\"]\n",
    "# @markdown * If set, it will be used to generate ground truth responses for the input examples. This is useful to migrate the prompt from a source model.\n",
    "TARGET_MODEL = \"gemini-1.5-flash-001\"  # @param [\"gemini-1.0-pro-001\", \"gemini-1.0-pro-002\", \"gemini-1.5-flash-001\", \"gemini-1.5-pro-001\", \"gemini-1.0-ultra-001\"]\n",
    "OPTIMIZATION_MODE = \"instruction_and_demo\"  # @param [\"instruction\", \"demonstration\", \"instruction_and_demo\"]\n",
    "EVAL_METRIC = \"question_answering_correctness\"  # @param [\"bleu\", \"coherence\", \"exact_match\", \"fluency\", \"groundedness\", \"text_quality\", \"verbosity\", \"rouge_1\", \"rouge_2\", \"rouge_l\", \"rouge_l_sum\", \"safety\", \"question_answering_correctness\", \"question_answering_quality\", \"summarization_quality\", \"tool_name_match\", \"tool_parameter_key_match\", \"tool_parameter_kv_match\", \"tool_call_valid\"] {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kO7fO0qTSNLs"
   },
   "source": [
    "# Step 4: Configure advanced optimization settings [Optional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fRHHTpaV4Xyo"
   },
   "outputs": [],
   "source": [
    "# @markdown **Instruction Optimization Configs**: <br/>\n",
    "NUM_INST_OPTIMIZATION_STEPS = 10  # @param {type:\"integer\"}\n",
    "NUM_TEMPLATES_PER_STEP = 2  # @param {type:\"integer\"}\n",
    "# @markdown * Number of prompt templates generated and evaluated at each optimization step.\n",
    "\n",
    "# @markdown **Demonstration Optimization Configs**: <br/>\n",
    "NUM_DEMO_OPTIMIZATION_STEPS = 10  # @param {type:\"integer\"}\n",
    "NUM_DEMO_PER_PROMPT = 3  # @param {type:\"integer\"}\n",
    "# @markdown * Number of the demonstrations to include in each prompt.\n",
    "\n",
    "# @markdown **Model Configs**: <br/>\n",
    "TARGET_MODEL_QPS = 3  # @param {type:\"integer\"}\n",
    "SOURCE_MODEL_QPS = 3  # @param {type:\"integer\"}\n",
    "EVAL_MODEL_QPS = 3  # @param {type:\"integer\"}\n",
    "# @markdown * The QPS for calling the eval model, which is currently gemini-1.5-pro-001.\n",
    "\n",
    "# @markdown **Multi-metric Configs**: <br/>\n",
    "# @markdown Use this section only if you need more than one metric for optimization. This will override the metric you picked above.\n",
    "EVAL_METRIC_1 = \"NA\"  # @param [\"NA\", \"bleu\", \"coherence\", \"exact_match\", \"fluency\", \"groundedness\", \"text_quality\", \"verbosity\", \"rouge_1\", \"rouge_2\", \"rouge_l\", \"rouge_l_sum\", \"safety\", \"question_answering_correctness\", \"question_answering_quality\", \"summarization_quality\", \"tool_name_match\", \"tool_parameter_key_match\", \"tool_parameter_kv_match\", \"tool_call_valid\"] {type:\"string\"}\n",
    "EVAL_METRIC_1_WEIGHT = 0.0  # @param {type:\"number\"}\n",
    "EVAL_METRIC_2 = \"NA\"  # @param [\"NA\", \"bleu\", \"coherence\", \"exact_match\", \"fluency\", \"groundedness\", \"text_quality\", \"verbosity\", \"rouge_1\", \"rouge_2\", \"rouge_l\", \"rouge_l_sum\", \"safety\", \"question_answering_correctness\", \"question_answering_quality\", \"summarization_quality\", \"tool_name_match\", \"tool_parameter_key_match\", \"tool_parameter_kv_match\", \"tool_call_valid\"] {type:\"string\"}\n",
    "EVAL_METRIC_2_WEIGHT = 0.0  # @param {type:\"number\"}\n",
    "EVAL_METRIC_3 = \"NA\"  # @param [\"NA\", \"bleu\", \"coherence\", \"exact_match\", \"fluency\", \"groundedness\", \"text_quality\", \"verbosity\", \"rouge_1\", \"rouge_2\", \"rouge_l\", \"rouge_l_sum\", \"safety\", \"question_answering_correctness\", \"question_answering_quality\", \"summarization_quality\", \"tool_name_match\", \"tool_parameter_key_match\", \"tool_parameter_kv_match\", \"tool_call_valid\"] {type:\"string\"}\n",
    "EVAL_METRIC_3_WEIGHT = 0.0  # @param {type:\"number\"}\n",
    "METRIC_AGGREGATION_TYPE = \"weighted_sum\"  # @param [\"weighted_sum\", \"weighted_average\"]\n",
    "\n",
    "# @markdown **Misc Configs**: <br/>\n",
    "PLACEHOLDER_TO_VALUE = \"{}\"  # @param\n",
    "# @markdown * This variable is used for long prompt optimization to not optimize parts of prompt identified by placeholders. It provides a mapping from the placeholder variables to their content. See link for details.\n",
    "RESPONSE_MIME_TYPE = \"application/json\"  # @param [\"text/plain\", \"application/json\"]\n",
    "# @markdown * This variable determines the format of the output for the target model. See link for details.\n",
    "TARGET_LANGUAGE = \"English\"  # @param [\"English\", \"French\", \"German\", \"Hebrew\", \"Hindi\", \"Japanese\", \"Korean\", \"Portuguese\", \"Simplified Chinese\", \"Spanish\", \"Traditional Chinese\"]\n",
    "# @markdown * The language of the system instruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7Mgb0EHSSFk"
   },
   "source": [
    "# Step 5: Run Prompt Optimizer\n",
    "A progress bar will appear to let you know how long the job takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8NvNLTfxPTf"
   },
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "display_name = f\"pt_{timestamp}\"\n",
    "\n",
    "in_colab_enterprise = \"GOOGLE_CLOUD_PROJECT\" in os.environ\n",
    "if not in_colab_enterprise:\n",
    "    auth.authenticate_user()\n",
    "\n",
    "label_enforced = vapo_lib.is_run_target_required(\n",
    "    [\n",
    "        EVAL_METRIC,\n",
    "        EVAL_METRIC_1,\n",
    "        EVAL_METRIC_2,\n",
    "        EVAL_METRIC_3,\n",
    "    ],\n",
    "    SOURCE_MODEL,\n",
    ")\n",
    "input_data_path = f\"{INPUT_DATA_PATH}\"\n",
    "vapo_lib.validate_prompt_and_data(\n",
    "    \"\\n\".join([SYSTEM_INSTRUCTION, PROMPT_TEMPLATE]),\n",
    "    input_data_path,\n",
    "    PLACEHOLDER_TO_VALUE,\n",
    "    label_enforced,\n",
    ")\n",
    "\n",
    "output_path = f\"{OUTPUT_PATH}/{display_name}\"\n",
    "\n",
    "params = {\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"num_steps\": NUM_INST_OPTIMIZATION_STEPS,\n",
    "    \"prompt_template\": SYSTEM_INSTRUCTION,\n",
    "    \"demo_and_query_template\": PROMPT_TEMPLATE,\n",
    "    \"target_model\": TARGET_MODEL,\n",
    "    \"target_model_qps\": TARGET_MODEL_QPS,\n",
    "    \"target_model_location\": LOCATION,\n",
    "    \"source_model\": SOURCE_MODEL,\n",
    "    \"source_model_qps\": SOURCE_MODEL_QPS,\n",
    "    \"source_model_location\": LOCATION,\n",
    "    \"eval_model_qps\": EVAL_MODEL_QPS,\n",
    "    \"eval_model_location\": LOCATION,\n",
    "    \"optimization_mode\": OPTIMIZATION_MODE,\n",
    "    \"num_demo_set_candidates\": NUM_DEMO_OPTIMIZATION_STEPS,\n",
    "    \"demo_set_size\": NUM_DEMO_PER_PROMPT,\n",
    "    \"aggregation_type\": METRIC_AGGREGATION_TYPE,\n",
    "    \"data_limit\": 50,\n",
    "    \"num_template_eval_per_step\": NUM_TEMPLATES_PER_STEP,\n",
    "    \"input_data_path\": input_data_path,\n",
    "    \"output_path\": output_path,\n",
    "    \"response_mime_type\": RESPONSE_MIME_TYPE,\n",
    "    \"language\": TARGET_LANGUAGE,\n",
    "    \"placeholder_to_content\": json.loads(PLACEHOLDER_TO_VALUE),\n",
    "}\n",
    "\n",
    "if EVAL_METRIC_1 == \"NA\":\n",
    "    params[\"eval_metrics_types\"] = [EVAL_METRIC]\n",
    "    params[\"eval_metrics_weights\"] = [1.0]\n",
    "else:\n",
    "    metrics = []\n",
    "    weights = []\n",
    "    for metric in [EVAL_METRIC_1, EVAL_METRIC_2, EVAL_METRIC_3]:\n",
    "        if metric == \"NA\":\n",
    "            break\n",
    "        metrics.append(metric)\n",
    "        weights.append(EVAL_METRIC_1_WEIGHT)\n",
    "    params[\"eval_metrics_types\"] = metrics\n",
    "    params[\"eval_metrics_weights\"] = weights\n",
    "\n",
    "job = vapo_lib.run_apd(params, OUTPUT_PATH, display_name)\n",
    "print(f\"Job ID: {job.name}\")\n",
    "\n",
    "progress_form = vapo_lib.ProgressForm(params)\n",
    "while progress_form.monitor_progress(job):\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lo5mcTzwSgBP"
   },
   "source": [
    "# Step 6: Inspect the Results\n",
    "For a clearer look at the specific responses generated by each prompt template during the optimization process, use the cell below. This will allow you to explore the actual output from the templates in more detail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1x6HSty759jY"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "\n",
    "RESULT_PATH = \"gs://prompt_design_demo\"  # @param {type:\"string\"}\n",
    "# @markdown * Specify a GCS path that contains artifacts of a single or multiple VAPO runs.\n",
    "\n",
    "results_ui = vapo_lib.ResultsUI(RESULT_PATH)\n",
    "\n",
    "results_df_html = \"\"\"\n",
    "<style>\n",
    "  .scrollable {\n",
    "    width: 100%;\n",
    "    height: 80px;\n",
    "    overflow-y: auto;\n",
    "    overflow-x: hidden;  /* Hide horizontal scrollbar */\n",
    "  }\n",
    "  tr:nth-child(odd) {\n",
    "    background: var(--colab-highlighted-surface-color);\n",
    "  }\n",
    "  tr:nth-child(even) {\n",
    "    background-color: var(--colab-primary-surface-color);\n",
    "  }\n",
    "  th {\n",
    "    background-color: var(--colab-highlighted-surface-color);\n",
    "  }\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(results_df_html))\n",
    "display(results_ui.get_container())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "vertex_ai_prompt_optimizer_ui.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
